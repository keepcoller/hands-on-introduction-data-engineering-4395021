[[34m2024-07-22T01:33:21.127+0000[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-07-22T01:33:21.128+0000[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[[34m2024-07-22T01:33:21.199+0000[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2024-07-22T01:33:21.200+0000[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[[34m2024-07-22T01:33:21.213+0000[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 25364[0m
[[34m2024-07-22T01:33:21.215+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-22T01:33:21.217+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-07-22T01:33:21.241+0000] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-07-22T01:38:21.403+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-22T01:40:14.518+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_dag to 2024-07-22 00:00:00+00:00, run_after=2024-07-23 00:00:00+00:00[0m
[[34m2024-07-22T01:40:14.617+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_dag.extract_task scheduled__2024-07-21T00:00:00+00:00 [scheduled]>[0m
[[34m2024-07-22T01:40:14.617+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-22T01:40:14.617+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_dag.extract_task scheduled__2024-07-21T00:00:00+00:00 [scheduled]>[0m
[[34m2024-07-22T01:40:14.619+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_dag', task_id='extract_task', run_id='scheduled__2024-07-21T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-22T01:40:14.619+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_dag', 'extract_task', 'scheduled__2024-07-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_dag.py'][0m
[[34m2024-07-22T01:40:14.643+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_dag', 'extract_task', 'scheduled__2024-07-21T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_dag.py'][0m
[[34m2024-07-22T01:40:15.867+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/extract_dag.py[0m
[[34m2024-07-22T01:40:16.199+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-07-22T01:40:16.200+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-07-22T01:40:16.212+0000[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-07-22T01:40:16.593+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_dag.extract_task scheduled__2024-07-21T00:00:00+00:00 [queued]> on host codespaces-8c1ce7[0m
[[34m2024-07-22T01:40:17.586+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_dag', task_id='extract_task', run_id='scheduled__2024-07-21T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-22T01:40:17.627+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_dag, task_id=extract_task, run_id=scheduled__2024-07-21T00:00:00+00:00, map_index=-1, run_start_date=2024-07-22 01:40:16.665461+00:00, run_end_date=2024-07-22 01:40:17.095124+00:00, run_duration=0.429663, state=success, executor_state=success, try_number=1, max_tries=0, job_id=2, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-22 01:40:14.618306+00:00, queued_by_job_id=1, pid=28475[0m
[[34m2024-07-22T01:40:17.788+0000[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun extract_dag @ 2024-07-21 00:00:00+00:00: scheduled__2024-07-21T00:00:00+00:00, state:running, queued_at: 2024-07-22 01:40:14.511685+00:00. externally triggered: False> successful[0m
[[34m2024-07-22T01:40:17.789+0000[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=extract_dag, execution_date=2024-07-21 00:00:00+00:00, run_id=scheduled__2024-07-21T00:00:00+00:00, run_start_date=2024-07-22 01:40:14.547683+00:00, run_end_date=2024-07-22 01:40:17.789096+00:00, run_duration=3.241413, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-07-21 00:00:00+00:00, data_interval_end=2024-07-22 00:00:00+00:00, dag_hash=cbf0b1a129994f8b1a3efa1ec5b63c5c[0m
[[34m2024-07-22T01:40:17.791+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for extract_dag to 2024-07-22 00:00:00+00:00, run_after=2024-07-23 00:00:00+00:00[0m
[[34m2024-07-22T01:40:49.971+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_dag.extract_task manual__2024-07-22T01:40:48.322991+00:00 [scheduled]>[0m
[[34m2024-07-22T01:40:49.971+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-22T01:40:49.971+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_dag.extract_task manual__2024-07-22T01:40:48.322991+00:00 [scheduled]>[0m
[[34m2024-07-22T01:40:49.973+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_dag', task_id='extract_task', run_id='manual__2024-07-22T01:40:48.322991+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-22T01:40:49.973+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_dag', 'extract_task', 'manual__2024-07-22T01:40:48.322991+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_dag.py'][0m
[[34m2024-07-22T01:40:50.001+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_dag', 'extract_task', 'manual__2024-07-22T01:40:48.322991+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_dag.py'][0m
[[34m2024-07-22T01:40:51.201+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/extract_dag.py[0m
[[34m2024-07-22T01:40:51.509+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-07-22T01:40:51.509+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-07-22T01:40:51.520+0000[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-07-22T01:40:51.864+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_dag.extract_task manual__2024-07-22T01:40:48.322991+00:00 [queued]> on host codespaces-8c1ce7[0m
[[34m2024-07-22T01:40:52.618+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_dag', task_id='extract_task', run_id='manual__2024-07-22T01:40:48.322991+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-22T01:40:52.621+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_dag, task_id=extract_task, run_id=manual__2024-07-22T01:40:48.322991+00:00, map_index=-1, run_start_date=2024-07-22 01:40:51.934825+00:00, run_end_date=2024-07-22 01:40:52.232707+00:00, run_duration=0.297882, state=success, executor_state=success, try_number=1, max_tries=0, job_id=3, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-22 01:40:49.972298+00:00, queued_by_job_id=1, pid=28708[0m
[[34m2024-07-22T01:40:52.753+0000[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun extract_dag @ 2024-07-22 01:40:48.322991+00:00: manual__2024-07-22T01:40:48.322991+00:00, state:running, queued_at: 2024-07-22 01:40:48.340733+00:00. externally triggered: True> successful[0m
[[34m2024-07-22T01:40:52.753+0000[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=extract_dag, execution_date=2024-07-22 01:40:48.322991+00:00, run_id=manual__2024-07-22T01:40:48.322991+00:00, run_start_date=2024-07-22 01:40:49.909871+00:00, run_end_date=2024-07-22 01:40:52.753882+00:00, run_duration=2.844011, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-07-21 01:40:48.322991+00:00, data_interval_end=2024-07-22 01:40:48.322991+00:00, dag_hash=cbf0b1a129994f8b1a3efa1ec5b63c5c[0m
[[34m2024-07-22T01:42:07.705+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_dag.extract_task manual__2024-07-22T01:42:06.338754+00:00 [scheduled]>[0m
[[34m2024-07-22T01:42:07.706+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-22T01:42:07.706+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_dag.extract_task manual__2024-07-22T01:42:06.338754+00:00 [scheduled]>[0m
[[34m2024-07-22T01:42:07.707+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_dag', task_id='extract_task', run_id='manual__2024-07-22T01:42:06.338754+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-22T01:42:07.708+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_dag', 'extract_task', 'manual__2024-07-22T01:42:06.338754+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_dag.py'][0m
[[34m2024-07-22T01:42:07.732+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_dag', 'extract_task', 'manual__2024-07-22T01:42:06.338754+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_dag.py'][0m
[[34m2024-07-22T01:42:09.011+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/extract_dag.py[0m
[[34m2024-07-22T01:42:09.324+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-07-22T01:42:09.324+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-07-22T01:42:09.342+0000[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-07-22T01:42:09.715+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_dag.extract_task manual__2024-07-22T01:42:06.338754+00:00 [queued]> on host codespaces-8c1ce7[0m
[[34m2024-07-22T01:42:10.506+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_dag', task_id='extract_task', run_id='manual__2024-07-22T01:42:06.338754+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-22T01:42:10.509+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_dag, task_id=extract_task, run_id=manual__2024-07-22T01:42:06.338754+00:00, map_index=-1, run_start_date=2024-07-22 01:42:09.802327+00:00, run_end_date=2024-07-22 01:42:10.025552+00:00, run_duration=0.223225, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=4, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-22 01:42:07.707034+00:00, queued_by_job_id=1, pid=29312[0m
[[34m2024-07-22T01:42:10.643+0000[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun extract_dag @ 2024-07-22 01:42:06.338754+00:00: manual__2024-07-22T01:42:06.338754+00:00, state:running, queued_at: 2024-07-22 01:42:06.347650+00:00. externally triggered: True> failed[0m
[[34m2024-07-22T01:42:10.643+0000[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=extract_dag, execution_date=2024-07-22 01:42:06.338754+00:00, run_id=manual__2024-07-22T01:42:06.338754+00:00, run_start_date=2024-07-22 01:42:07.643299+00:00, run_end_date=2024-07-22 01:42:10.643700+00:00, run_duration=3.000401, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-07-21 01:42:06.338754+00:00, data_interval_end=2024-07-22 01:42:06.338754+00:00, dag_hash=af5c9f499b161ecdb1c12ed40da55644[0m
[[34m2024-07-22T01:43:21.490+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-07-22T01:43:35.685+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_dag.extract_task manual__2024-07-22T01:43:34.292984+00:00 [scheduled]>[0m
[[34m2024-07-22T01:43:35.686+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG extract_dag has 0/16 running and queued tasks[0m
[[34m2024-07-22T01:43:35.686+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_dag.extract_task manual__2024-07-22T01:43:34.292984+00:00 [scheduled]>[0m
[[34m2024-07-22T01:43:35.687+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='extract_dag', task_id='extract_task', run_id='manual__2024-07-22T01:43:34.292984+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-07-22T01:43:35.688+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_dag', 'extract_task', 'manual__2024-07-22T01:43:34.292984+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_dag.py'][0m
[[34m2024-07-22T01:43:35.717+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_dag', 'extract_task', 'manual__2024-07-22T01:43:34.292984+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_dag.py'][0m
[[34m2024-07-22T01:43:36.986+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/extract_dag.py[0m
[[34m2024-07-22T01:43:37.305+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-07-22T01:43:37.305+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-07-22T01:43:37.316+0000[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-07-22T01:43:37.670+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: extract_dag.extract_task manual__2024-07-22T01:43:34.292984+00:00 [queued]> on host codespaces-8c1ce7[0m
[[34m2024-07-22T01:43:38.456+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_dag', task_id='extract_task', run_id='manual__2024-07-22T01:43:34.292984+00:00', try_number=1, map_index=-1)[0m
[[34m2024-07-22T01:43:38.459+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=extract_dag, task_id=extract_task, run_id=manual__2024-07-22T01:43:34.292984+00:00, map_index=-1, run_start_date=2024-07-22 01:43:37.743206+00:00, run_end_date=2024-07-22 01:43:38.028366+00:00, run_duration=0.28516, state=success, executor_state=success, try_number=1, max_tries=0, job_id=5, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-07-22 01:43:35.686866+00:00, queued_by_job_id=1, pid=29952[0m
[[34m2024-07-22T01:43:38.592+0000[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun extract_dag @ 2024-07-22 01:43:34.292984+00:00: manual__2024-07-22T01:43:34.292984+00:00, state:running, queued_at: 2024-07-22 01:43:34.304439+00:00. externally triggered: True> successful[0m
[[34m2024-07-22T01:43:38.593+0000[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=extract_dag, execution_date=2024-07-22 01:43:34.292984+00:00, run_id=manual__2024-07-22T01:43:34.292984+00:00, run_start_date=2024-07-22 01:43:35.625117+00:00, run_end_date=2024-07-22 01:43:38.593202+00:00, run_duration=2.968085, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-07-21 01:43:34.292984+00:00, data_interval_end=2024-07-22 01:43:34.292984+00:00, dag_hash=6cb802d2a7af9e20a4c678f76accb2fb[0m
